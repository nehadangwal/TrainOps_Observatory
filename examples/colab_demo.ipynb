# TrainOps Observatory - GPU Training Demo
# Run this notebook in Google Colab with GPU enabled
# Runtime -> Change runtime type -> GPU

"""
# TrainOps Observatory - Google Colab Demo

This notebook demonstrates how to use TrainOps Observatory to monitor and optimize
GPU training workflows. We'll train a ResNet model on CIFAR-10 and see real-time
bottleneck detection and cost optimization recommendations.

## Setup Instructions:
1. Enable GPU: Runtime -> Change runtime type -> GPU (T4)
2. Run all cells in order
3. See bottleneck analysis at the end

Estimated time: 10 minutes
"""

# ============================================================================
# SECTION 1: Environment Setup
# ============================================================================

print("üöÄ Setting up TrainOps Observatory...")

# Install dependencies
!pip install torch torchvision tqdm psutil pynvml gputil -q

print("‚úÖ Setup complete!")

# ============================================================================
# SECTION 2: Simplified TrainOps Monitor (No Backend)
# ============================================================================

import torch
import time
import psutil
import json
from datetime import datetime
from collections import defaultdict

try:
    import pynvml
    pynvml.nvmlInit()
    GPU_AVAILABLE = True
except:
    GPU_AVAILABLE = False

class SimpleTrainOpsMonitor:
    """Simplified TrainOps monitor for Colab - stores metrics in memory"""
    
    def __init__(self, run_name, project="colab_demo", instance_type="colab_t4"):
        self.run_name = run_name
        self.project = project
        self.instance_type = instance_type
        self.run_id = f"{run_name}_{int(time.time())}"
        
        self.start_time = time.time()
        self.step_count = 0
        self.epoch_count = 0
        
        # Store all metrics in memory
        self.step_metrics = []
        self.epoch_metrics = []
        self.system_metrics = []
        
        if GPU_AVAILABLE:
            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        print(f"üìä TrainOps Monitor initialized!")
        print(f"   Run ID: {self.run_id}")
        print(f"   Project: {self.project}")
        print(f"   Instance: {self.instance_type}")
    
    def _collect_system_metrics(self):
        """Collect current system metrics"""
        metrics = {
            'timestamp': time.time(),
            'cpu_percent': psutil.cpu_percent(interval=0.1),
            'ram_percent': psutil.virtual_memory().percent,
        }
        
        if GPU_AVAILABLE:
            try:
                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)
                mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
                
                metrics['gpu_utilization'] = gpu_util.gpu
                metrics['gpu_memory_used'] = mem_info.used / 1e9  # GB
                metrics['gpu_memory_total'] = mem_info.total / 1e9  # GB
                metrics['gpu_memory_percent'] = (mem_info.used / mem_info.total) * 100
            except:
                metrics['gpu_utilization'] = 0
                metrics['gpu_memory_used'] = 0
                metrics['gpu_memory_total'] = 0
                metrics['gpu_memory_percent'] = 0
        
        return metrics
    
    def track_training(self, func):
        """Decorator to track training function"""
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        return wrapper
    
    def log_step(self, **kwargs):
        """Log metrics for a training step"""
        self.step_count += 1
        
        # Collect system metrics
        sys_metrics = self._collect_system_metrics()
        
        # Combine with user metrics
        step_data = {
            'step': self.step_count,
            'timestamp': time.time(),
            **sys_metrics,
            **kwargs
        }
        
        self.step_metrics.append(step_data)
        self.system_metrics.append(sys_metrics)
    
    def log_epoch(self, epoch, **kwargs):
        """Log metrics for an epoch"""
        self.epoch_count += 1
        
        epoch_data = {
            'epoch': epoch,
            'timestamp': time.time(),
            **kwargs
        }
        
        self.epoch_metrics.append(epoch_data)
    
    def finish(self):
        """Finish monitoring and generate report"""
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
        
        print(f"\n‚úÖ Training complete! Duration: {self.duration:.2f}s")
        print(f"üìä Collected {len(self.step_metrics)} step metrics")
    
    def analyze(self):
        """Analyze metrics and detect bottlenecks"""
        if not self.system_metrics:
            print("‚ö†Ô∏è  No metrics collected yet")
            return
        
        # Calculate averages
        avg_gpu = sum(m.get('gpu_utilization', 0) for m in self.system_metrics) / len(self.system_metrics)
        avg_cpu = sum(m.get('cpu_percent', 0) for m in self.system_metrics) / len(self.system_metrics)
        avg_ram = sum(m.get('ram_percent', 0) for m in self.system_metrics) / len(self.system_metrics)
        
        # Calculate throughput if available
        throughputs = [m.get('samples_per_sec', 0) for m in self.step_metrics if 'samples_per_sec' in m]
        avg_throughput = sum(throughputs) / len(throughputs) if throughputs else 0
        
        print("\n" + "="*70)
        print("üìä TRAINING ANALYSIS")
        print("="*70)
        
        print(f"\n‚è±Ô∏è  Duration: {self.duration:.2f} seconds ({self.duration/60:.2f} minutes)")
        print(f"üìà Total Steps: {self.step_count}")
        print(f"üìâ Total Epochs: {self.epoch_count}")
        
        print(f"\nüìä Average System Metrics:")
        print(f"   GPU Utilization: {avg_gpu:.1f}%")
        print(f"   CPU Utilization: {avg_cpu:.1f}%")
        print(f"   RAM Usage: {avg_ram:.1f}%")
        if avg_throughput > 0:
            print(f"   Throughput: {avg_throughput:.1f} samples/sec")
        
        # Bottleneck detection
        print(f"\nüîç BOTTLENECK ANALYSIS:")
        print("="*70)
        
        bottlenecks_found = False
        
        # I/O Bottleneck
        if avg_gpu < 60:
            bottlenecks_found = True
            print("\nüö® I/O BOTTLENECK DETECTED (High Severity)")
            print("‚îÅ" * 70)
            print(f"\nYour GPU is only {avg_gpu:.1f}% utilized.")
            print("This means the GPU is waiting for data most of the time.")
            print("\nüí° ROOT CAUSE:")
            print("   ‚Ä¢ Data loading (DataLoader) can't keep up with GPU")
            print("   ‚Ä¢ The CPU is preparing batches too slowly")
            print("\nüíä RECOMMENDED FIX:")
            print("   Change your DataLoader configuration:")
            print("   ")
            print("   ‚ùå Current:  DataLoader(..., num_workers=0)")
            print("   ‚úÖ Change to: DataLoader(..., num_workers=2 or 4, pin_memory=True)")
            print("\nüìà EXPECTED IMPROVEMENT:")
            
            # Calculate potential savings
            current_time_min = self.duration / 60
            optimized_time_min = current_time_min * 0.6  # 40% improvement
            time_saved_min = current_time_min - optimized_time_min
            
            print(f"   ‚Ä¢ GPU Utilization: {avg_gpu:.1f}% ‚Üí 75-85%")
            print(f"   ‚Ä¢ Training Time: {current_time_min:.1f}min ‚Üí {optimized_time_min:.1f}min")
            print(f"   ‚Ä¢ Time Saved: {time_saved_min:.1f} minutes (-40%)")
            
            # Cost calculation
            cost_per_hour = 0.50  # Colab T4 approximate cost
            current_cost = (current_time_min / 60) * cost_per_hour
            optimized_cost = (optimized_time_min / 60) * cost_per_hour
            savings = current_cost - optimized_cost
            
            print(f"\nüí∞ COST IMPACT (Colab T4 @ ${cost_per_hour:.2f}/hr):")
            print(f"   ‚Ä¢ Current Cost: ${current_cost:.3f}")
            print(f"   ‚Ä¢ Optimized Cost: ${optimized_cost:.3f}")
            print(f"   ‚Ä¢ Savings per run: ${savings:.3f}")
            print(f"   ‚Ä¢ If 10 runs/month: ${savings*10:.2f}/month saved")
            
        elif avg_gpu >= 60 and avg_gpu < 80:
            print("\n‚úÖ GPU Utilization is GOOD but can be optimized")
            print("‚îÅ" * 70)
            print(f"\nCurrent GPU utilization: {avg_gpu:.1f}%")
            print("Target: 80-95% for maximum efficiency")
            print("\nüí° SUGGESTIONS:")
            print("   ‚Ä¢ Try increasing batch size (if memory allows)")
            print("   ‚Ä¢ Consider adding more DataLoader workers")
            print("   ‚Ä¢ Enable mixed precision training (torch.cuda.amp)")
            
        else:
            print("\n‚úÖ EXCELLENT GPU UTILIZATION!")
            print("‚îÅ" * 70)
            print(f"\nGPU is efficiently utilized at {avg_gpu:.1f}%")
            print("Your training pipeline is well optimized! üéâ")
        
        # CPU Bottleneck
        if avg_cpu > 80 and avg_gpu < 70:
            bottlenecks_found = True
            print("\n‚ö†Ô∏è  CPU BOTTLENECK DETECTED")
            print("‚îÅ" * 70)
            print(f"\nCPU is at {avg_cpu:.1f}% while GPU is only {avg_gpu:.1f}%")
            print("\nüí° SUGGESTIONS:")
            print("   ‚Ä¢ Reduce data preprocessing complexity")
            print("   ‚Ä¢ Reduce num_workers (CPU overload)")
            print("   ‚Ä¢ Consider pre-processing data offline")
        
        # Memory Warning
        if avg_ram > 85:
            print("\n‚ö†Ô∏è  HIGH MEMORY USAGE")
            print("‚îÅ" * 70)
            print(f"\nRAM usage is at {avg_ram:.1f}%")
            print("\nüí° SUGGESTIONS:")
            print("   ‚Ä¢ Reduce batch size")
            print("   ‚Ä¢ Reduce num_workers")
            print("   ‚Ä¢ Clear unused variables")
        
        if not bottlenecks_found and avg_gpu >= 80:
            print("\nüéâ NO BOTTLENECKS DETECTED!")
            print("Your training configuration is optimal!")
        
        print("\n" + "="*70)
        
        return {
            'avg_gpu_util': avg_gpu,
            'avg_cpu_util': avg_cpu,
            'avg_ram': avg_ram,
            'duration': self.duration,
            'bottleneck_detected': avg_gpu < 60
        }

print("‚úÖ TrainOps Monitor ready!")

# ============================================================================
# SECTION 3: Check GPU Availability
# ============================================================================

print("\nüîç Checking GPU availability...")
print(f"GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("‚ö†Ô∏è  No GPU detected! Please enable GPU in Runtime settings.")
    print("   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU")

# ============================================================================
# SECTION 4: Training Configuration
# ============================================================================

import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm

# ‚ö†Ô∏è START WITH SUBOPTIMAL SETTINGS TO DEMONSTRATE BOTTLENECK
BATCH_SIZE = 128
NUM_EPOCHS = 2  # Reduced for faster demo
NUM_WORKERS = 0  # ‚ö†Ô∏è This will cause I/O bottleneck!
LEARNING_RATE = 0.1

print("\n" + "="*70)
print("üéØ TRAINING CONFIGURATION")
print("="*70)
print(f"  Batch Size: {BATCH_SIZE}")
print(f"  Epochs: {NUM_EPOCHS}")
print(f"  DataLoader Workers: {NUM_WORKERS} ‚ö†Ô∏è  (INTENTIONALLY LOW)")
print(f"  Learning Rate: {LEARNING_RATE}")
print("\n‚ö†Ô∏è  NOTE: Starting with num_workers=0 to demonstrate I/O bottleneck")
print("   We'll show how to fix this in the analysis!")
print("="*70)

# Initialize TrainOps Monitor
monitor = SimpleTrainOpsMonitor(
    run_name="cifar10_resnet_colab",
    project="colab_demo",
    instance_type="colab_t4"
)

# ============================================================================
# SECTION 5: Prepare Data
# ============================================================================

print("\nüì¶ Preparing CIFAR-10 dataset...")

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=True,
    download=True, 
    transform=transform_train
)

trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=True if torch.cuda.is_available() else False
)

testset = torchvision.datasets.CIFAR10(
    root='./data',
    train=False,
    download=True,
    transform=transform_test
)

testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"‚úÖ Dataset loaded: {len(trainset)} training samples, {len(testset)} test samples")

# ============================================================================
# SECTION 6: Define Model
# ============================================================================

print("\nüèóÔ∏è  Building ResNet-18 model...")

model = torchvision.models.resnet18(num_classes=10)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

print(f"‚úÖ Model ready on device: {device}")
print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")

# ============================================================================
# SECTION 7: Training Loop with TrainOps Monitoring
# ============================================================================

print("\nüèãÔ∏è  Starting training with TrainOps monitoring...")
print("   Watch the GPU utilization - it will likely be low!\n")

@monitor.track_training
def train_epoch(epoch):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    epoch_start = time.time()
    pbar = tqdm(trainloader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")
    
    for batch_idx, (inputs, targets) in enumerate(pbar):
        batch_start = time.time()
        
        inputs, targets = inputs.to(device), targets.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Calculate metrics
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        # Calculate throughput
        batch_time = time.time() - batch_start
        samples_per_sec = BATCH_SIZE / batch_time if batch_time > 0 else 0
        
        # Log to TrainOps
        monitor.log_step(
            loss=loss.item(),
            accuracy=100. * correct / total,
            learning_rate=optimizer.param_groups[0]['lr'],
            samples_per_sec=samples_per_sec
        )
        
        # Update progress bar
        pbar.set_postfix({
            'loss': f'{running_loss/(batch_idx+1):.3f}',
            'acc': f'{100.*correct/total:.2f}%',
            'samples/s': f'{samples_per_sec:.1f}'
        })
    
    epoch_time = time.time() - epoch_start
    return running_loss / len(trainloader), 100. * correct / total, epoch_time

def validate():
    """Validate on test set"""
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in tqdm(testloader, desc="Validating", leave=False):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return test_loss / len(testloader), 100. * correct / total

# Training loop
best_acc = 0
for epoch in range(NUM_EPOCHS):
    train_loss, train_acc, epoch_time = train_epoch(epoch)
    test_loss, test_acc = validate()
    
    # Log epoch metrics
    monitor.log_epoch(
        epoch=epoch,
        train_loss=train_loss,
        train_accuracy=train_acc,
        test_loss=test_loss,
        test_accuracy=test_acc,
        epoch_time=epoch_time
    )
    
    scheduler.step()
    
    if test_acc > best_acc:
        best_acc = test_acc
    
    print(f"\nEpoch {epoch+1}/{NUM_EPOCHS} Summary:")
    print(f"  Time: {epoch_time:.2f}s")
    print(f"  Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%")
    print(f"  Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%")
    print(f"  Best Acc: {best_acc:.2f}%\n")

# Finish monitoring
monitor.finish()

# ============================================================================
# SECTION 8: Analyze Results and Show Recommendations
# ============================================================================

analysis = monitor.analyze()

# ============================================================================
# SECTION 9: Next Steps
# ============================================================================

print("\n" + "="*70)
print("üî¨ EXPERIMENT: TEST THE OPTIMIZATION")
print("="*70)
print("""
Want to see the improvement? Try these changes:

1. Scroll to the "Training Configuration" section above
2. Change: NUM_WORKERS = 0
   To:     NUM_WORKERS = 2  (or 4 if you have more CPU)
3. Re-run all cells from that point

Expected Results:
  ‚úÖ GPU utilization: ~45% ‚Üí 75-85%
  ‚úÖ Training time: Reduced by 30-40%
  ‚úÖ Throughput: 2-3x faster

This is the power of TrainOps Observatory - it helps you identify
and fix bottlenecks to save time and money!
""")

print("\n‚ú® Demo complete!")
print("\nüìö Learn more:")
print("   GitHub: https://github.com/nehadangwal/TrainOps_Observatory")
print("   Documentation: See repo README.md")